{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cda2cdf8-fd70-474b-a319-59a0269ac9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2302  in train\n",
      "576  in test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40613/359593953.py:49: DeprecationWarning: FLIP_TOP_BOTTOM is deprecated and will be removed in Pillow 10 (2023-07-01). Use Transpose.FLIP_TOP_BOTTOM instead.\n",
      "  Image.open(map_path + map_ext).transpose(Image.FLIP_TOP_BOTTOM)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from enum import Enum, unique\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.multiprocessing import freeze_support\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from pit.dynamics.kinematic_bicycle import Bicycle\n",
    "from pit.integration import Euler, RK4\n",
    "\n",
    "freeze_support()\n",
    "\n",
    "\n",
    "class Curvature(Enum):\n",
    "    NO_CURVATURE = False\n",
    "    CURVATURE = True\n",
    "\n",
    "\n",
    "@unique\n",
    "class Method(Enum):\n",
    "    PIMP = 0\n",
    "    LSTM = 1\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def _get_map_points(map_path, map_ext):\n",
    "    with open(map_path + \".yaml\", \"r\") as yaml_stream:\n",
    "        try:\n",
    "            map_metadata = yaml.safe_load(yaml_stream)\n",
    "            map_resolution = map_metadata[\"resolution\"]\n",
    "            origin = map_metadata[\"origin\"]\n",
    "            origin_x = origin[0]\n",
    "            origin_y = origin[1]\n",
    "        except yaml.YAMLError as ex:\n",
    "            print(ex)\n",
    "    map_img = np.array(\n",
    "        Image.open(map_path + map_ext).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "    ).astype(np.float64)\n",
    "    map_height = map_img.shape[0]\n",
    "    map_width = map_img.shape[1]\n",
    "\n",
    "    # convert map pixels to coordinates\n",
    "    range_x = np.arange(map_width)\n",
    "    range_y = np.arange(map_height)\n",
    "    map_x, map_y = np.meshgrid(range_x, range_y)\n",
    "    map_x = (map_x * map_resolution + origin_x).flatten()\n",
    "    map_y = (map_y * map_resolution + origin_y).flatten()\n",
    "    map_z = np.zeros(map_y.shape)\n",
    "    map_coords = np.vstack((map_x, map_y, map_z))\n",
    "\n",
    "    # mask and only leave the obstacle points\n",
    "    map_mask = map_img == 0.0\n",
    "    map_mask_flat = map_mask.flatten()\n",
    "    map_points = map_coords[:, map_mask_flat].T\n",
    "    return map_points[:, 0], map_points[:, 1]\n",
    "\n",
    "\n",
    "map_x, map_y = _get_map_points(\"../data_generation/track_config/Spielberg_map\", \".png\")\n",
    "\n",
    "train_frame = pd.read_pickle(\"../../data/train_data.pkl\")\n",
    "test_frame = pd.read_pickle(\"../../data/test_data.pkl\")\n",
    "full_frame = pd.read_pickle(\"../../data/final_data.pkl\")\n",
    "\n",
    "no_race_train_frame = train_frame[\n",
    "    train_frame[\"selected_lane\"].apply(\n",
    "        lambda x: True if x in [\"left\", \"center\", \"right\"] else False\n",
    "    )\n",
    "]\n",
    "no_race_test_frame = test_frame[\n",
    "    test_frame[\"selected_lane\"].apply(\n",
    "        lambda x: True if x in [\"left\", \"center\", \"right\"] else False\n",
    "    )\n",
    "]\n",
    "race_test_frame = full_frame[\n",
    "    full_frame[\"selected_lane\"].apply(lambda x: True if x in [\"race\"] else False)\n",
    "]\n",
    "\n",
    "RACE_SELECTION = [2246, 2329, 2711, 2596, 2465, 2365, 2805, 2554, 2266]\n",
    "\n",
    "\n",
    "class TraceRelativeDataset(Dataset):\n",
    "    def __init__(self, dataframe, curve=False):\n",
    "        self.dataframe = dataframe\n",
    "        self.curve = curve\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.curve:\n",
    "            key = \"input\"\n",
    "        else:\n",
    "            key = \"input_no_curve\"\n",
    "\n",
    "        try:\n",
    "            inputs = torch.tensor(\n",
    "                np.array(self.dataframe.iloc[idx][key].to_list()), dtype=torch.float32\n",
    "            )\n",
    "            last_pose = torch.tensor(\n",
    "                np.array(self.dataframe.iloc[idx][\"last_pose\"].to_list()),\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "            target = torch.tensor(\n",
    "                np.array(self.dataframe.iloc[idx][\"target\"].to_list()),\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "        except AttributeError as v:\n",
    "            inputs = torch.tensor(self.dataframe.iloc[idx][key], dtype=torch.float32)\n",
    "            last_pose = torch.tensor(\n",
    "                self.dataframe.iloc[idx][\"last_pose\"], dtype=torch.float32\n",
    "            )\n",
    "            target = torch.tensor(\n",
    "                self.dataframe.iloc[idx][\"target\"], dtype=torch.float32\n",
    "            )\n",
    "        return inputs, last_pose, target\n",
    "\n",
    "\n",
    "train_dataset = TraceRelativeDataset(train_frame, curve=False)\n",
    "test_dataset = TraceRelativeDataset(test_frame, curve=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "\n",
    "print(len(train_dataset), \" in train\")\n",
    "print(len(test_dataset), \" in test\")\n",
    "\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=32, horizon=60, num_layers=2):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.horizon = horizon\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers=num_layers, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.hidden2output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            # nn.Linear(hidden_dim//2, hidden_dim//2),\n",
    "            # nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 2),\n",
    "            nn.Linear(hidden_dim // 2, 3 * horizon),\n",
    "        )\n",
    "        print(\"WARNING: 2 layer LSTM + 2 layer decoder\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lstm_out, _ = self.lstm(inputs)\n",
    "        output = self.hidden2output(lstm_out)\n",
    "        output = output[:, -1].reshape((inputs.shape[0], self.horizon, 3))\n",
    "        return output\n",
    "\n",
    "    def predict(self, inputs, last_poses, horizon=60):\n",
    "        residuals = self.forward(inputs)\n",
    "        last_poses = last_poses.to(DEVICE)\n",
    "        outputs = torch.tile(\n",
    "            last_poses[:, :3].reshape(last_poses.shape[0], 1, 3), (1, 60, 1)\n",
    "        )\n",
    "        outputs = residuals + outputs\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class LSTMPredictorBicycle(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim=3, hidden_dim=32, control_outputs=1, num_layers=2, horizon=60\n",
    "    ):\n",
    "        super(LSTMPredictorBicycle, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.control_outputs = control_outputs\n",
    "        self.horizon = horizon\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers=num_layers, batch_first=True\n",
    "        )\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.Linear(hidden_dim // 2, control_outputs * 2 + 1),\n",
    "        )\n",
    "\n",
    "        self.dynamics = Bicycle(0.3302)\n",
    "        self.integrator = Euler(self.dynamics, timestep=0.1)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        lstm_out, _ = self.lstm(inputs)\n",
    "        output = self.hidden2output(lstm_out)\n",
    "        scaled_output = list()\n",
    "        scaled_output.append(F.softplus(output[:, :, 0]))\n",
    "        for step in range(self.control_outputs):\n",
    "            scaled_output.append(\n",
    "                torch.tanh(output[:, :, (step * 2) + 1]) * np.pi\n",
    "            )  # Steering\n",
    "            scaled_output.append(output[:, :, (step * 2) + 2])  # Acceleration\n",
    "        output = torch.dstack(scaled_output)\n",
    "        return output\n",
    "\n",
    "    def predict(self, inputs, last_poses):\n",
    "        # Compute LSTM output\n",
    "        controls = self.forward(inputs)[:, -1]  # Take last prediction\n",
    "        last_poses = last_poses.to(DEVICE)\n",
    "        BATCHES = controls.shape[0]\n",
    "        states = []  # torch.zeros((81, 4))\n",
    "        L = 0.3302\n",
    "        TS = 0.1\n",
    "        X, Y, THETA, V = 0, 1, 2, 3\n",
    "        CDIMS = 2\n",
    "        HORIZON = 60\n",
    "        state = torch.zeros(\n",
    "            (\n",
    "                BATCHES,\n",
    "                4,\n",
    "            ),\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        state[:, X] = last_poses[:, 0]\n",
    "        state[:, Y] = last_poses[:, 1]\n",
    "        state[:, THETA] = last_poses[:, 2]\n",
    "        state[:, V] = controls[:, 0]\n",
    "        \n",
    "        real_controls = torch.zeros((BATCHES, self.horizon, CDIMS), device=DEVICE)\n",
    "        step_length = self.horizon // self.control_outputs\n",
    "        for i in range(1, self.horizon):\n",
    "            step = min((i) // step_length, self.control_outputs - 1)\n",
    "            real_controls[:,i] = controls[:, [(step * CDIMS) + 1,(step * CDIMS) + 2]]\n",
    "        \n",
    "        trace = self.integrator(state, real_controls)\n",
    "        return trace\n",
    "\n",
    "\n",
    "def create_debug_plot(\n",
    "    net,\n",
    "    train_dataset: TraceRelativeDataset = train_dataset,\n",
    "    test_dataset: TraceRelativeDataset = test_dataset,\n",
    "    curvature: Curvature = Curvature.NO_CURVATURE,\n",
    "    selection: list = [1912, 2465, 533, 905, 277, 1665, 2395, 61, 1054],\n",
    "):\n",
    "    if selection is None:\n",
    "        selection = np.random.choice(len(full_frame), 9)\n",
    "    inputs, last_poses, targets = train_dataset[: len(selection)]\n",
    "    inputs = torch.zeros_like(inputs, dtype=torch.float32, device=DEVICE)\n",
    "    last_poses = torch.zeros_like(last_poses, dtype=torch.float32, device=DEVICE)\n",
    "    for i, DATA_IDX in enumerate(selection):\n",
    "        data_in_train = True if DATA_IDX in train_frame.index else False\n",
    "        dframe = full_frame\n",
    "        if curvature is Curvature.NO_CURVATURE:\n",
    "            inputs[i] = torch.tensor(\n",
    "                dframe.loc[DATA_IDX][\"input_no_curve\"],\n",
    "                dtype=torch.float32,\n",
    "                device=DEVICE,\n",
    "            )\n",
    "        else:\n",
    "            inputs[i] = torch.tensor(\n",
    "                dframe.loc[DATA_IDX][\"input\"], dtype=torch.float32, device=DEVICE\n",
    "            )\n",
    "        last_poses[i] = torch.tensor(\n",
    "            dframe.loc[DATA_IDX][\"last_pose\"], dtype=torch.float32, device=DEVICE\n",
    "        )\n",
    "        targets[i] = torch.tensor(\n",
    "            dframe.loc[DATA_IDX][\"target\"], dtype=torch.float32, device=DEVICE\n",
    "        )\n",
    "    outputs = net.predict(inputs, last_poses).detach().cpu().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(10, 10), dpi=300)\n",
    "    for idx, DATA_IDX in enumerate(selection):\n",
    "        data_in_train = True if DATA_IDX in train_frame.index else False\n",
    "        dframe = full_frame\n",
    "        (linput,) = axs[idx // 3, idx % 3].plot(\n",
    "            dframe.loc[DATA_IDX][\"input\"][:, 0],\n",
    "            dframe.loc[DATA_IDX][\"input\"][:, 1],\n",
    "            marker=\".\",\n",
    "            label=\"Input\",\n",
    "        )\n",
    "        (ltarget,) = axs[idx // 3, idx % 3].plot(\n",
    "            dframe.loc[DATA_IDX][\"target\"][:, 0],\n",
    "            dframe.loc[DATA_IDX][\"target\"][:, 1],\n",
    "            marker=\".\",\n",
    "            label=\"Target\",\n",
    "        )\n",
    "        (lpred,) = axs[idx // 3, idx % 3].plot(\n",
    "            outputs[idx, :, 0],\n",
    "            outputs[idx, :, 1],\n",
    "            marker=\"o\",\n",
    "            mfc=\"none\",\n",
    "            label=\"Prediction\",\n",
    "        )\n",
    "        xlim, ylim = np.average(axs[idx // 3, idx % 3].get_xlim()), np.average(\n",
    "            axs[idx // 3, idx % 3].get_ylim()\n",
    "        )\n",
    "        axs[idx // 3, idx % 3].scatter(map_x, map_y, marker=\".\", color=\"black\")\n",
    "        axs[idx // 3, idx % 3].set(\n",
    "            xlim=(xlim - 3.5, xlim + 3.5),\n",
    "            ylim=(ylim - 3.5, ylim + 3.5),\n",
    "            aspect=1.0,\n",
    "            adjustable=\"box\",\n",
    "            yticklabels=[],\n",
    "            xticklabels=[],\n",
    "        )\n",
    "        axs[idx // 3, idx % 3].set_title(\n",
    "            \"{}:{}:{}\".format(\n",
    "                \"Train\" if data_in_train else \"Test\",\n",
    "                DATA_IDX,\n",
    "                dframe.loc[DATA_IDX][\"selected_lane\"],\n",
    "            )\n",
    "        )\n",
    "    fig.suptitle(\"Manually Selected Traces\")\n",
    "    fig.legend(handles=[linput, ltarget, lpred])\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f42424e-7829-46ce-a299-5dfa9a272518",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTMPredictorBicycle()\n",
    "net = net.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d231ab5-93ae-4184-b750-c7c524213ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=1\n",
    "curriculum=False\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "for input_data, last_pose, target_data in train_dataloader:\n",
    "    net.zero_grad()\n",
    "    input_data = input_data.to(DEVICE)\n",
    "    last_pose = last_pose.to(DEVICE)\n",
    "    outp = net.predict(input_data, last_pose)\n",
    "    target_data = target_data.to(DEVICE)\n",
    "\n",
    "    loss = custom_loss_func(outp, target_data)\n",
    "    end_index = 30\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    fde = final_displacement_error(outp, target_data).detach().cpu().numpy()\n",
    "    ade = average_displacement_error(outp, target_data).detach().cpu().numpy()\n",
    "    cum_train_loss = loss.item()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df62397a-1c05-498f-b55c-61a4b7ada834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.770482540130615"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e2b07d7-1400-47c8-b24c-9ffbc9390e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ea91b00-ddfa-4c1d-93f4-401cc4d9f04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 60, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b404f1eb-4101-496c-a3f7-7fd79b62fd97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 60, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "893f43c3-9c4f-486c-b6d5-718d35bf9b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_func(prediction, target):\n",
    "    loss = F.smooth_l1_loss(prediction[..., :2], target[..., :2])\n",
    "    loss += 4 * F.smooth_l1_loss(prediction[..., 2], target[..., 2])\n",
    "    # loss += 10*output[0]**2 if output[0]<0 else 0\n",
    "    # loss += 2*torch.linalg.norm(output)**2\n",
    "    return loss\n",
    "\n",
    "\n",
    "def average_displacement_error(prediction, target):\n",
    "    loss = torch.linalg.norm(prediction[..., :2] - target[..., :2], dim=1)\n",
    "    ade = torch.mean(loss, dim=0)\n",
    "    return torch.mean(ade)\n",
    "\n",
    "\n",
    "def final_displacement_error(prediction, target):\n",
    "    loss = torch.linalg.norm(prediction[..., :2] - target[..., :2], dim=1)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "800f88a9-0f0c-4eaa-ad72-276869595d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ade, fde, loss = list(), list(), list()\n",
    "for batch in range(input_data.shape[0]):\n",
    "    ade.append(average_displacement_error(outp[batch], target_data[batch]).detach().cpu().numpy())\n",
    "    fde.append(final_displacement_error(outp[batch], target_data[batch]).detach().cpu().numpy())\n",
    "    loss.append(custom_loss_func(outp[batch], target_data[batch]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "27be5b5a-4783-45ee-9bdb-fab1e2e0486d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b574edf4-1eea-48b6-be21-5ab3a05b5d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0050, 0.0101, 0.0172, 0.0271, 0.0407, 0.0581, 0.0799, 0.1062, 0.1372,\n",
       "        0.1729, 0.2132, 0.2585, 0.3085, 0.3638, 0.4240, 0.4896, 0.5609, 0.6377,\n",
       "        0.7203, 0.8083, 0.9021, 1.0012, 1.1051, 1.2140, 1.3274, 1.4454, 1.5672,\n",
       "        1.6930, 1.8227, 1.9554, 2.0909, 2.2285, 2.3677, 2.5076, 2.6471, 2.7858,\n",
       "        2.9226, 3.0568, 3.1873, 3.3134, 3.4343, 3.5488, 3.6559, 3.7545, 3.8436,\n",
       "        3.9219, 3.9884, 4.0422, 4.0823, 4.1082, 4.1189, 4.1142, 4.0938, 4.0574,\n",
       "        4.0054, 3.9380, 3.8565, 3.7620, 3.6567, 3.5434], device='cuda:1',\n",
       "       grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.norm(outp[batch][..., :2] - target_data[batch][..., :2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f8e3b19-a9c4-4233-97e9-320f697064ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = [f'cuda:{i}' for i in range(3)]\n",
    "procs_per_gpu = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "80058e5a-dbcb-4e56-9605-a1f0ceeb4f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cuda:0', 'cuda:1', 'cuda:2', 'cuda:0', 'cuda:1', 'cuda:2']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus*procs_per_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10756b3-28a3-4f26-877e-13e306511f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:VMP]",
   "language": "python",
   "name": "conda-env-VMP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
