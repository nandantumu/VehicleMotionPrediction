{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "from PIL import Image\n",
    "import yaml\n",
    "import av2\n",
    "import numpy\n",
    "from pathlib import Path\n",
    "from av2.datasets.motion_forecasting import scenario_serialization\n",
    "from av2.datasets.motion_forecasting.data_schema import TrackCategory, ObjectType, ObjectState, Track\n",
    "from av2.map.map_api import ArgoverseStaticMap\n",
    "from av2.utils.typing import NDArrayFloat, NDArrayInt\n",
    "\n",
    "from typing import Final, List, Optional, Sequence, Set, Tuple\n",
    "from tensorboardX import SummaryWriter\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum, unique\n",
    "\n",
    "class Curvature(Enum):\n",
    "    NO_CURVATURE = False\n",
    "    CURVATURE = True\n",
    "\n",
    "def state_to_vec(state:ObjectState) -> np.ndarray:\n",
    "    return np.array([state.position[0], state.position[1], state.heading])\n",
    "\n",
    "def trace_to_vec(track:Track) -> np.ndarray:\n",
    "    states = list()\n",
    "    for state in track.object_states:\n",
    "        states.append(state_to_vec(state))\n",
    "    states = np.array(states)\n",
    "    try:\n",
    "        assert states.shape == (110,3)\n",
    "        return states\n",
    "    except AssertionError as a:\n",
    "        tqdm.write(\"ID:{}\\tShape:{}\".format(track.track_id, states.shape))\n",
    "        raise a\n",
    "\n",
    "def get_scenario_id(path:Path):\n",
    "    return path.stem.split(\"_\")[-1]\n",
    "\n",
    "def get_map_path(path:Path):\n",
    "    return path.parents[0] / f\"log_map_archive_{get_scenario_id(path)}.json\"\n",
    "\n",
    "def get_tracks_for_path(path:Path):\n",
    "    tracks = list()\n",
    "    scene = scenario_serialization.load_argoverse_scenario_parquet(path)\n",
    "    for track in scene.tracks:\n",
    "        if track.category == TrackCategory.SCORED_TRACK and track.object_type == ObjectType.VEHICLE and len(track.object_states) == 110:\n",
    "            tracks.append((track, get_map_path(path)))\n",
    "    return tracks\n",
    "        \n",
    "class AV2MotionPredictionDataset(Dataset):\n",
    "    def __init__(self, scenario_files, n_jobs=32):\n",
    "        self.scenario_files = scenario_files\n",
    "        #self.packed_traces = [self._get_tracks_for_path(path) for path in tqdm(self.scenario_files)]\n",
    "        self.packed_traces = Parallel(n_jobs=32)(delayed(get_tracks_for_path)(path) for path in tqdm(self.scenario_files))\n",
    "        self.packed_traces = [item for sublist in self.packed_traces for item in sublist]\n",
    "        self.maps = [map_path for track, map_path in self.packed_traces]\n",
    "        self.traces = Parallel(n_jobs=32)(delayed(trace_to_vec)(trace) for trace, path in tqdm(self.packed_traces))\n",
    "        self.traces = np.array(self.traces)\n",
    "        del self.packed_traces\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.traces.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):      \n",
    "        inputs = torch.tensor(self.traces[idx, :50], dtype=torch.float32)\n",
    "        last_pose = torch.tensor(self.traces[idx, 49], dtype=torch.float32)\n",
    "        target = torch.tensor(self.traces[idx, 50:], dtype=torch.float32)\n",
    "        return inputs, last_pose, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path_to_train_dir = Path('/PIMP/Argoverse2/train')\n",
    "path_to_test_dir = Path('/PIMP/Argoverse2/test')\n",
    "path_to_val_dir = Path('/PIMP/Argoverse2/val')\n",
    "train_scenario_files = list(path_to_train_dir.rglob(\"*.parquet\"))\n",
    "test_scenario_files = list(path_to_test_dir.rglob(\"*.parquet\"))\n",
    "val_scenario_files = list(path_to_val_dir.rglob(\"*.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = train_scenario_files[10]\n",
    "traces = AV2MotionPredictionDataset._get_tracks_for_path(path)\n",
    "trace = traces[0]\n",
    "\n",
    "trace_to_vec(trace).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_dataset = AV2MotionPredictionDataset(test_scenario_files)\n",
    "print(test_dataset.traces.shape)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5a532656e64bedbe029d8d049409f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e058778fd72f41dbb6250e7efc28c018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63554 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63554, 110, 3)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = AV2MotionPredictionDataset(val_scenario_files)\n",
    "print(val_dataset.traces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3914f88a6ba34166a8ea531c4802d70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a500181cecf47adab2de4117fbe9b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506528, 110, 3)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AV2MotionPredictionDataset(train_scenario_files)\n",
    "print(train_dataset.traces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506528"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63554"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bicycle_model_eval(inputs, last_poses):\n",
    "    # This version takes in an input of dim 5\n",
    "    BATCHES = inputs.shape[0]\n",
    "    states = []  # torch.zeros((81, 4))\n",
    "    L = 0.3302\n",
    "    TS = 0.1\n",
    "    X, Y, THETA, V = 0, 1, 2, 3\n",
    "    state = torch.zeros((BATCHES, 4,))\n",
    "    state[:,X] = last_poses[:,0]\n",
    "    state[:,Y] = last_poses[:,1]\n",
    "    state[:,THETA] = last_poses[:,2]\n",
    "    state[:,V] = inputs[:,0]\n",
    "    states.append(state)\n",
    "    for i in range(1,81):\n",
    "        # Advance bicycle model\n",
    "        state = torch.zeros((BATCHES, 4,))\n",
    "        state[:,X] = states[i-1][:,X] + (TS * states[i-1][:,V] * torch.cos(states[i-1][:,THETA]))\n",
    "        state[:,Y] = states[i-1][:,Y] + (TS * states[i-1][:,V] * torch.sin(states[i-1][:,THETA]))\n",
    "        state[:,THETA] = states[i-1][:,THETA] + (TS * (states[i-1][:,V] * torch.tan(inputs[:,1])) / L)\n",
    "        state[:,V] = states[i-1][:,V] + TS*inputs[:,2]\n",
    "        states.append(state)\n",
    "    trace = torch.dstack(states).movedim((0,1,2), (0,2,1))\n",
    "    trace = trace[:,1:,:3]\n",
    "    return trace\n",
    "\n",
    "def custom_loss_func(prediction, target):\n",
    "    loss = F.smooth_l1_loss(prediction[:,:,:2], target[:,:,:2])\n",
    "    loss += F.smooth_l1_loss(prediction[:,:,2], target[:,:,2])\n",
    "    #loss += 10*output[0]**2 if output[0]<0 else 0\n",
    "    #loss += 2*torch.linalg.norm(output)**2\n",
    "    return loss\n",
    "\n",
    "def average_displacement_error(prediction, target):\n",
    "    loss = torch.linalg.norm(prediction[:,:,:2]-target[:,:,:2], dim=2)\n",
    "    ade = torch.mean(loss, dim=0)\n",
    "    return torch.mean(ade)\n",
    "\n",
    "def final_displacement_error(prediction, target):\n",
    "    loss = torch.linalg.norm(prediction[:,-1,:2]-target[:,-1,:2], dim=1)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "class LSTMPredictorBicycle(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=32, control_outputs=1):\n",
    "        super(LSTMPredictorBicycle, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.control_outputs = control_outputs\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=4, batch_first=True)\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//2),\n",
    "            nn.Linear(hidden_dim//2, 1+control_outputs*2)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        lstm_out, _ = self.lstm(inputs)\n",
    "        output = self.hidden2output(lstm_out)\n",
    "        scaled_output = list()\n",
    "        scaled_output.append(F.softplus(output[:,:,0]))\n",
    "        for step in range(self.control_outputs):\n",
    "            scaled_output.append(torch.tanh(output[:,:,(step*2)+1])*np.pi)\n",
    "            scaled_output.append(output[:,:,(step*2)+2])\n",
    "        output = torch.dstack(scaled_output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, inputs, last_poses):\n",
    "        return self.predict(inputs, last_poses, horizon=60)\n",
    "    \n",
    "    def predict(self, inputs, last_poses, horizon=60):\n",
    "        # Compute LSTM output\n",
    "        controls = self._forward(inputs)[:, -1]  # Take last prediction\n",
    "        last_poses = last_poses.to(DEVICE)\n",
    "        BATCHES = controls.shape[0]\n",
    "        states = []  # torch.zeros((81, 4))\n",
    "        L = 3.5\n",
    "        TS = 0.1\n",
    "        X, Y, THETA, V = 0, 1, 2, 3\n",
    "        CDIMS = 2\n",
    "        state = torch.zeros((BATCHES, 4,), device=DEVICE)\n",
    "        state[:,X] = last_poses[:,0]\n",
    "        state[:,Y] = last_poses[:,1]\n",
    "        state[:,THETA] = last_poses[:,2]\n",
    "        state[:,V] = controls[:,0]\n",
    "        states.append(state)\n",
    "        step_length = horizon//self.control_outputs\n",
    "        for i in range(1,horizon+1):\n",
    "            # Advance bicycle model\n",
    "            step = min((i-1)//step_length, self.control_outputs-1)\n",
    "            state = torch.zeros((BATCHES, 4,), device=DEVICE)\n",
    "            state[:,X] = states[i-1][:,X] + (TS * states[i-1][:,V] * torch.cos(states[i-1][:,THETA]))\n",
    "            state[:,Y] = states[i-1][:,Y] + (TS * states[i-1][:,V] * torch.sin(states[i-1][:,THETA]))\n",
    "            state[:,THETA] = states[i-1][:,THETA] + (TS * (states[i-1][:,V] * torch.tan(controls[:,(step*CDIMS)+1])) / L)\n",
    "            state[:,V] = states[i-1][:,V] + TS*controls[:,(step*CDIMS)+2]\n",
    "            states.append(state)\n",
    "        trace = torch.dstack(states).movedim((0,1,2), (0,2,1))\n",
    "        trace = trace[:,1:,:3]\n",
    "        return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60//8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DRIVABLE_AREA_COLOR = \"#7A7A7A\"\n",
    "_LANE_SEGMENT_COLOR = \"#E0E0E0\"\n",
    "def _plot_static_map_elements(ax:plt.Axes, static_map: ArgoverseStaticMap, show_ped_xings: bool = False) -> None:\n",
    "    \"\"\"Plot all static map elements associated with an Argoverse scenario.\n",
    "    Args:\n",
    "        static_map: Static map containing elements to be plotted.\n",
    "        show_ped_xings: Configures whether pedestrian crossings should be plotted.\n",
    "    \"\"\"\n",
    "    # Plot drivable areas\n",
    "    for drivable_area in static_map.vector_drivable_areas.values():\n",
    "        _plot_polygons(ax, [drivable_area.xyz], alpha=0.5, color=_DRIVABLE_AREA_COLOR)\n",
    "\n",
    "    # Plot lane segments\n",
    "    for lane_segment in static_map.vector_lane_segments.values():\n",
    "        _plot_polylines(ax,\n",
    "            [\n",
    "                lane_segment.left_lane_boundary.xyz,\n",
    "                lane_segment.right_lane_boundary.xyz,\n",
    "            ],\n",
    "            line_width=0.5,\n",
    "            color=_LANE_SEGMENT_COLOR,\n",
    "        )\n",
    "\n",
    "    # Plot pedestrian crossings\n",
    "    if show_ped_xings:\n",
    "        for ped_xing in static_map.vector_pedestrian_crossings.values():\n",
    "            _plot_polylines(ax,[ped_xing.edge1.xyz, ped_xing.edge2.xyz], alpha=1.0, color=_LANE_SEGMENT_COLOR)\n",
    "\n",
    "def _plot_polylines(\n",
    "    ax:plt.Axes,\n",
    "    polylines: Sequence[NDArrayFloat],\n",
    "    *,\n",
    "    style: str = \"-\",\n",
    "    line_width: float = 1.0,\n",
    "    alpha: float = 1.0,\n",
    "    color: str = \"r\",\n",
    ") -> None:\n",
    "    \"\"\"Plot a group of polylines with the specified config.\n",
    "    Args:\n",
    "        polylines: Collection of (N, 2) polylines to plot.\n",
    "        style: Style of the line to plot (e.g. `-` for solid, `--` for dashed)\n",
    "        line_width: Desired width for the plotted lines.\n",
    "        alpha: Desired alpha for the plotted lines.\n",
    "        color: Desired color for the plotted lines.\n",
    "    \"\"\"\n",
    "    for polyline in polylines:\n",
    "        ax.plot(polyline[:, 0], polyline[:, 1], style, linewidth=line_width, color=color, alpha=alpha)\n",
    "\n",
    "def _plot_polygons(ax:plt.Axes, polygons: Sequence[NDArrayFloat], *, alpha: float = 1.0, color: str = \"r\") -> None:\n",
    "    \"\"\"Plot a group of filled polygons with the specified config.\n",
    "    Args:\n",
    "        polygons: Collection of polygons specified by (N,2) arrays of vertices.\n",
    "        alpha: Desired alpha for the polygon fill.\n",
    "        color: Desired color for the polygon.\n",
    "    \"\"\"\n",
    "    for polygon in polygons:\n",
    "        ax.fill(polygon[:, 0], polygon[:, 1], color=color, alpha=alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_debug_plot(net, dataset:AV2MotionPredictionDataset=val_dataset):\n",
    "    selection = np.random.choice(len(dataset), size=9).tolist()\n",
    "    map_paths = [dataset.maps[index] for index in selection]\n",
    "    inputs, last_poses, targets = train_dataset[selection]\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    last_poses = last_poses.to(DEVICE)\n",
    "    targets = targets.to(DEVICE)\n",
    "    \n",
    "    outputs = net(inputs, last_poses).detach().cpu().numpy()\n",
    "    inputs = inputs.detach().cpu().numpy()\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(3,3,figsize=(10,10), dpi=300)\n",
    "    for idx, DATA_IDX in enumerate(selection):\n",
    "        linput, = axs[idx//3, idx%3].plot(inputs[idx,:,0], inputs[idx,:,1], marker='.', label=\"Input\")\n",
    "        ltarget, = axs[idx//3, idx%3].plot(targets[idx,:,0], targets[idx,:,1], marker='.', label=\"Target\")\n",
    "        lpred, = axs[idx//3, idx%3].plot(outputs[idx,:,0], outputs[idx,:,1], marker='x', label=\"Prediction\")\n",
    "        xlim, ylim = np.average(axs[idx//3, idx%3].get_xlim()), np.average(axs[idx//3, idx%3].get_ylim())\n",
    "        \n",
    "        #st_map = ArgoverseStaticMap.from_json(map_paths[idx])\n",
    "        #_plot_static_map_elements(axs[idx//3, idx%3], st_map, True)\n",
    "        \n",
    "        axs[idx//3, idx%3].set(xmargin=3,#xlim=(xlim-2,xlim+2),\n",
    "                               ymargin=3,#ylim=(ylim-2,ylim+2),\n",
    "                               aspect=1.0,\n",
    "                               adjustable='box',\n",
    "                               yticklabels=[],\n",
    "                               xticklabels=[]\n",
    "                              )\n",
    "        axs[idx//3, idx%3].set_title(\"{}\".format(DATA_IDX))\n",
    "    fig.suptitle(\"Randomly Selected Traces\")\n",
    "    fig.legend(handles=[linput, ltarget, lpred])\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfig, ax = plt.subplots(figsize=(7,7), dpi=150);\n",
    "ax.set_aspect(1.0, adjustable='box');\n",
    "ax.set_title(\"TEST\");\n",
    "ax.plot(other_outp[0,:,0].cpu().detach().numpy(), other_outp[0,:,1].cpu().detach().numpy(), marker='o', label='Predicted');\n",
    "ax.plot(target[0,:,0].detach().numpy(), target[0,:,1].detach().numpy(), marker='o', label='Target');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7), dpi=150);\n",
    "ax.set_aspect(1.0, adjustable='box');\n",
    "ax.set_title(\"TEST\");\n",
    "ax.plot(other_outp[0,:,0].cpu().detach().numpy(), other_outp[0,:,1].cpu().detach().numpy(), marker='o', label='Predicted')\n",
    "ax.plot(target[0,:10,0].detach().numpy(), target[0,:10,1].detach().numpy(), marker='o', label='Target')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(control_outputs:int, hidden_dim:int=32, epochs=100, batch_size=1024, lr=1e-3):\n",
    "    directory = f'runs/av2/bicycle-{control_outputs}-{hidden_dim}/'\n",
    "    writer = SummaryWriter(directory)\n",
    "\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    \n",
    "    # Training Loop\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    net = LSTMPredictorBicycle(input_dim=3,\n",
    "                               hidden_dim=hidden_dim,\n",
    "                               control_outputs=control_outputs)\n",
    "    net.to(DEVICE)\n",
    "    net = torch.nn.DataParallel(net, device_ids=[1, 1, 1])\n",
    "    pytorch_total_params = sum(p.numel() for p in net.parameters())\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    train_losses = list()\n",
    "    val_losses = list()\n",
    "    train_ades, val_ades = list(), list()\n",
    "    train_fdes, val_fdes = list(), list()\n",
    "\n",
    "    i=0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        cum_train_loss = 0.0\n",
    "        net.train()\n",
    "        fde, ade = list(), list()\n",
    "\n",
    "        for input_data, last_pose, target_data in tqdm(train_dataloader):\n",
    "            net.zero_grad()\n",
    "            input_data = input_data.to(DEVICE)\n",
    "            last_pose = last_pose.to(DEVICE)\n",
    "            outp = net(input_data, last_pose)\n",
    "            target_data = target_data.to(DEVICE)\n",
    "            loss = custom_loss_func(outp, target_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                fde.append(final_displacement_error(outp, target_data).cpu().numpy())\n",
    "                ade.append(average_displacement_error(outp, target_data).cpu().numpy())\n",
    "            cum_train_loss += loss.item()\n",
    "            \n",
    "            writer.add_scalar(\"batch/loss\", loss.item(), i)\n",
    "            writer.add_scalar(\"batch/ade\", ade[-1], i)\n",
    "            writer.add_scalar(\"batch/fde\", fde[-1], i)\n",
    "            i+=1\n",
    "\n",
    "        train_fig, train_ax = create_debug_plot(net, train_dataset)\n",
    "        cum_train_loss /= len(train_dataset)\n",
    "        train_losses.append(cum_train_loss)\n",
    "        ade, fde = np.mean(ade), np.mean(fde)\n",
    "        writer.add_scalar(\"ADE/train\", ade, epoch)\n",
    "        writer.add_scalar(\"FDE/train\", fde, epoch)\n",
    "        train_ades.append(ade)\n",
    "        train_fdes.append(fde)\n",
    "        cum_val_loss = 0.0\n",
    "        fde, ade = list(), list()\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for input_data, last_pose, target_data in tqdm(val_dataloader):\n",
    "                input_data = input_data.to(DEVICE)\n",
    "                last_pose = last_pose.to(DEVICE)\n",
    "                outp = net(input_data, last_pose)\n",
    "                target_data = target_data.to(DEVICE)\n",
    "                loss = custom_loss_func(outp, target_data)\n",
    "                cum_val_loss += loss.item()\n",
    "                fde.append(final_displacement_error(outp, target_data).cpu().numpy())\n",
    "                ade.append(average_displacement_error(outp, target_data).cpu().numpy())\n",
    "            val_fig, val_ax = create_debug_plot(net, val_dataset)\n",
    "        cum_val_loss /= len(val_dataset)\n",
    "        val_losses.append(cum_val_loss)\n",
    "        ade, fde = np.mean(ade), np.mean(fde)\n",
    "        writer.add_scalar(\"ADE/val\", ade, epoch)\n",
    "        writer.add_scalar(\"FDE/val\", fde, epoch)\n",
    "        val_ades.append(ade)\n",
    "        val_fdes.append(fde)\n",
    "        if cum_val_loss <= min(val_losses):\n",
    "            torch.save(net.state_dict(), f\"{directory}/best_model.pt\")\n",
    "        writer.add_scalar(\"loss/train\", cum_train_loss, epoch)\n",
    "        writer.add_scalar(\"loss/val\", cum_val_loss, epoch)\n",
    "        writer.add_figure(\"train/example_fig\", train_fig, epoch)\n",
    "        writer.add_figure(\"val/example_fig\", val_fig, epoch)\n",
    "        tqdm.write(f\"Epoch {epoch} | Train Loss: {cum_train_loss} | Val Loss: {cum_val_loss}\")\n",
    "    return {\n",
    "        'control_outputs': control_ouptut,\n",
    "        'curvature': curvature,\n",
    "        'hidden_dims': hidden_dims,\n",
    "        'training_loss': cum_train_loss,\n",
    "        'val_loss': cum_val_loss,\n",
    "        'ade': ade,\n",
    "        'fde': fde,\n",
    "        'params': pytorch_total_params\n",
    "    }\n",
    "\n",
    "def train_catch(*args, **kwargs):\n",
    "    import traceback\n",
    "    try:\n",
    "        train(*args, **kwargs)\n",
    "    except Exception as e:\n",
    "        print(\"Exception Occurred:\")\n",
    "        print(e)\n",
    "        traceback.print_exc()\n",
    "        print(\"--------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'control_outputs': [1,2,4,10,20,30,40,60],\n",
    "    #'curvature': [Curvature.NO_CURVATURE, Curvature.CURVATURE],\n",
    "    'hidden_dims': [32, 64],\n",
    "}\n",
    "hyp_packages = list()\n",
    "for control_ouptut in hyperparameters['control_outputs']:\n",
    "    for hidden_dims in hyperparameters['hidden_dims']:\n",
    "        hyp_packages.append({\n",
    "            'control_outputs': control_ouptut,\n",
    "            'hidden_dims': hidden_dims,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5496dcf58b4482ac46e56ade474621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    }
   ],
   "source": [
    "train(6, 128, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in net.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
